{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Churn_Modelling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GZs8gTlsMTLv",
        "colab_type": "code",
        "outputId": "2852da4d-cd92-4097-9592-d2deaf6bb466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "abizeQf9PePG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Author : Karthik Vikram"
      ]
    },
    {
      "metadata": {
        "id": "X5X9sRpDelv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <center>Data Preprocessing</center>"
      ]
    },
    {
      "metadata": {
        "id": "wwKbzxzjepWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Classification template\n",
        "\n",
        "# Importing the libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Deep_Learning_A_Z/Part 1/Artificial_Neural_Networks/Churn_Modelling.csv')\n",
        "X = dataset.iloc[:,3:-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FBXInC9n20W",
        "colab_type": "code",
        "outputId": "cf61f03d-c903-4360-ed5e-59943cae3c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "dataset.head(n=8)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>15574012</td>\n",
              "      <td>Chu</td>\n",
              "      <td>645</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Male</td>\n",
              "      <td>44</td>\n",
              "      <td>8</td>\n",
              "      <td>113755.78</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>149756.71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>15592531</td>\n",
              "      <td>Bartlett</td>\n",
              "      <td>822</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10062.80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>15656148</td>\n",
              "      <td>Obinna</td>\n",
              "      <td>376</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Female</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>115046.74</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>119346.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
              "0          1    15634602  Hargrave          619    France  Female   42   \n",
              "1          2    15647311      Hill          608     Spain  Female   41   \n",
              "2          3    15619304      Onio          502    France  Female   42   \n",
              "3          4    15701354      Boni          699    France  Female   39   \n",
              "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
              "5          6    15574012       Chu          645     Spain    Male   44   \n",
              "6          7    15592531  Bartlett          822    France    Male   50   \n",
              "7          8    15656148    Obinna          376   Germany  Female   29   \n",
              "\n",
              "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0       2       0.00              1          1               1   \n",
              "1       1   83807.86              1          0               1   \n",
              "2       8  159660.80              3          1               0   \n",
              "3       1       0.00              2          0               0   \n",
              "4       2  125510.82              1          1               1   \n",
              "5       8  113755.78              2          1               0   \n",
              "6       7       0.00              2          1               1   \n",
              "7       4  115046.74              4          1               0   \n",
              "\n",
              "   EstimatedSalary  Exited  \n",
              "0        101348.88       1  \n",
              "1        112542.58       0  \n",
              "2        113931.57       1  \n",
              "3         93826.63       0  \n",
              "4         79084.10       0  \n",
              "5        149756.71       1  \n",
              "6         10062.80       0  \n",
              "7        119346.88       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "K3g12Xc2oJiG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examining the column names, we can see that the columns {RowNumber,CustomerId,Surname} have no significance in determining if the customer leaves the bank. Hence we ignore and omit these columns from the dataset. Now the shape of our dataset is **10000 x 10** with the extra columns removed"
      ]
    },
    {
      "metadata": {
        "id": "ik0EZv5WqGG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center> <h3>Encoding the Data</h3></center>\n",
        "\n",
        "\n",
        "The Column names Geography and Gender name are strings. ANN works best on numbers. Closer examination shows that the data is collected from only three countries - France, Germany and Spain (i.e. 3 categorical data). We can <i>OneHotEncode</i> it into 3 columns of binaries. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The gender column has only two possible values - Either male or female so we directly convert it to a single column binary "
      ]
    },
    {
      "metadata": {
        "id": "5Cuvkqurnu6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "57b01d89-7601-47a4-83cc-6a86e59021d8"
      },
      "cell_type": "code",
      "source": [
        "#Encoding Catergorical Data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X1.fit_transform(X[:, 1])\n",
        "\n",
        "labelencoder_X2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X1.fit_transform(X[:, 2])\n",
        "\n",
        "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder.fit_transform(X).toarray()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8qvJrjShvDnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3><center>Removing the Dummy Variable</center></h3>\n",
        "\n",
        "Dummy Variables : Consider there is a column with 3 categorical values A,B,C. We can OneHotEncode it into 3 columns i.e. A column, B column , C column where if the value is A for an entry; we will put a 1 in the A column and 0 in the B and C column. The same applies for values B and C.\n",
        "\n",
        "We can achieve the same results using 2 columns ; i.e.\n",
        "\n",
        "B | B column 1 ; C column 0\n",
        "\n",
        "C | B column 0 ; C column 1\n",
        "\n",
        "take a note here,\n",
        "\n",
        "A | B column 0 ; C column 0 - We have got a representation for A without the third column. Now A column is rudimentary (dummy). We can remove it.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We have converted the Gerography column containing 3 categories into 3 columns of binaries. From the explanation given above we can remove any one column from the 3, the representation remains the same."
      ]
    },
    {
      "metadata": {
        "id": "KVbEsFIGvBOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Removing the dummy variable column\n",
        "X = X[:,1:] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDI0N07mp7UY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZw7BrduLUYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center><h4>Feature Scaling</h4></center>\n",
        "\n",
        "From the we can see that the numeric data in different columns are not in the same range. While training the model, such vast difference makes some columns of data insignificant while setting the weights of synases. To overcome this we scale the inputs into relative scale. i.e. we take their log values for training.\n"
      ]
    },
    {
      "metadata": {
        "id": "NlSpSMYfLTLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ta7-f0yqOrvO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Aritificial Neural Network\n",
        "\n",
        "The first layer after the input layers is the ReLu activation layer. The weights of the synapses are initialized with numbers close to zero. \n",
        "\n",
        "I have used Dropout Regularization to prevent oerfitting. The parameter rate=0.1 specifies that 10% of the nuerons will be disengaged while training in order to bring about better correlation between the different nuerons and the features in the dataset.\n",
        "\n",
        "The 'adam' SGD optimizer function. Since the prediction of the model has to be Yes or No, I have used a sigmoid layer at the last, that will return the probability of the output being a yes or no.\n",
        "\n",
        "The loss function I have used is the binary_crossentropy function which will be good for binary output predictions which are in the format of probabilities.\n",
        "\n",
        "The accuracy metrics for synapses' weight adjustement. \n"
      ]
    },
    {
      "metadata": {
        "id": "AdZa5qx1dSXB",
        "colab_type": "code",
        "outputId": "5df75ca7-92fe-47a8-b4b0-496c93bec4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3514
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Fitting classifier to the Training set\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Create your classifier here\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'\n",
        "                     ,input_dim=11))\n",
        "classifier.add(Dropout(rate=0.1))\n",
        "\n",
        "#Second Hidden layer\n",
        "classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'))\n",
        "classifier.add(Dropout(rate=0.1))\n",
        "\n",
        "#Output Layer\n",
        "classifier.add(Dense(output_dim=1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "classifier.add(Dropout(rate=0.1))\n",
        "# We use sigmoid so that we get a probability of the customer\n",
        "# leaving the bank and the probabilty of not leaving the bank\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "#Training the model, the weights are adjusted once in 10 samples\n",
        "classifier.fit(X_train,y_train,batch_size=10,epochs=100)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=6)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"sigmoid\", units=1)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 1s 183us/step - loss: 0.7813 - acc: 0.7235\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7447 - acc: 0.7960\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7229 - acc: 0.7964\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7217 - acc: 0.8054\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7067 - acc: 0.8207\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.7190 - acc: 0.8217\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6980 - acc: 0.8250\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6867 - acc: 0.8254\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6730 - acc: 0.8277\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7126 - acc: 0.8241\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 1s 101us/step - loss: 0.6935 - acc: 0.8271\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7309 - acc: 0.8267\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6684 - acc: 0.8269\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6875 - acc: 0.8260\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6826 - acc: 0.8266\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6748 - acc: 0.8281\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6965 - acc: 0.8270\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7143 - acc: 0.8265\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6920 - acc: 0.8246\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7154 - acc: 0.8254\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7044 - acc: 0.8257\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6753 - acc: 0.8271\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 1s 102us/step - loss: 0.7108 - acc: 0.8236\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6768 - acc: 0.8290\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.7125 - acc: 0.8261\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6941 - acc: 0.8231\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 1s 108us/step - loss: 0.6956 - acc: 0.8260\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6864 - acc: 0.8247\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7028 - acc: 0.8284\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.6857 - acc: 0.8282\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 1s 107us/step - loss: 0.6618 - acc: 0.8282\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.7302 - acc: 0.8285\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 1s 108us/step - loss: 0.7099 - acc: 0.8242\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6603 - acc: 0.8279\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 1s 108us/step - loss: 0.6490 - acc: 0.8289\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6829 - acc: 0.8275\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7204 - acc: 0.8284\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 1s 109us/step - loss: 0.7298 - acc: 0.8270\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 1s 107us/step - loss: 0.6814 - acc: 0.8299\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 1s 108us/step - loss: 0.7000 - acc: 0.8284\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6695 - acc: 0.8299\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7189 - acc: 0.8296\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7345 - acc: 0.8282\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 1s 109us/step - loss: 0.6731 - acc: 0.8286\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7025 - acc: 0.8289\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6972 - acc: 0.8285\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7043 - acc: 0.8257\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6861 - acc: 0.8251\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6751 - acc: 0.8295\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7022 - acc: 0.8272\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6598 - acc: 0.8284\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6794 - acc: 0.8289\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6739 - acc: 0.8292\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.7147 - acc: 0.8276\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 1s 107us/step - loss: 0.6742 - acc: 0.8275\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.6786 - acc: 0.8285\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 1s 107us/step - loss: 0.6959 - acc: 0.8295\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.7109 - acc: 0.8294\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 1s 107us/step - loss: 0.6954 - acc: 0.8286\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6626 - acc: 0.8319\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 1s 109us/step - loss: 0.6679 - acc: 0.8291\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.6834 - acc: 0.8280\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.7448 - acc: 0.8280\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6648 - acc: 0.8287\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6931 - acc: 0.8305\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7092 - acc: 0.8280\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6993 - acc: 0.8292\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6805 - acc: 0.8289\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6642 - acc: 0.8296\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6994 - acc: 0.8285\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6719 - acc: 0.8294\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7125 - acc: 0.8271\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7034 - acc: 0.8261\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7550 - acc: 0.8276\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 1s 102us/step - loss: 0.6826 - acc: 0.8299\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.6860 - acc: 0.8277\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.6907 - acc: 0.8295\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6833 - acc: 0.8319\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.7053 - acc: 0.8285\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7126 - acc: 0.8277\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7075 - acc: 0.8287\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7001 - acc: 0.8291\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.6843 - acc: 0.8292\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6979 - acc: 0.8287\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6963 - acc: 0.8289\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7007 - acc: 0.8275\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7131 - acc: 0.8272\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 1s 103us/step - loss: 0.6495 - acc: 0.8325\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6957 - acc: 0.8302\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7096 - acc: 0.8275\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6610 - acc: 0.8302\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.7181 - acc: 0.8289\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6654 - acc: 0.8292\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 1s 106us/step - loss: 0.6928 - acc: 0.8297\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6818 - acc: 0.8300\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 1s 109us/step - loss: 0.6713 - acc: 0.8309\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 1s 108us/step - loss: 0.6938 - acc: 0.8300\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.6578 - acc: 0.8316\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 1s 104us/step - loss: 0.6566 - acc: 0.8312\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 1s 105us/step - loss: 0.7061 - acc: 0.8321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f10bd3e0710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "x54rZ5v_-H_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center><h4>Running the model on the Test Set</h4></center>\n",
        "\n",
        "The model predicts the probabilities of whether a certain customer leaves the bank or not. I have converted the probabilities as a binary output by keeping a threshold of 0.5 . i.e. if the probability is greater than 0.5 the customer leaves the bank.\n"
      ]
    },
    {
      "metadata": {
        "id": "amar6otl9quL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred>0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHMnKwhFiLLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "25ad0544-357c-4d49-a300-e9ad07f6e77f"
      },
      "cell_type": "code",
      "source": [
        "cm"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1557,   38],\n",
              "       [ 286,  119]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "uTTV-IqMPPOG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network Evaluation\n",
        "\n",
        "The accuracies obtained above change with each run. I have used K-fold (10 folds) cross validation for getting a better insight of the model's performance. The mean of all the accuracies obtained in 10 runs of model will be more accurate. From the standard deviation of the accuracy values we can see that our model has low bias and low variance which means our model can produce consistent and accurate predictions."
      ]
    },
    {
      "metadata": {
        "id": "ptW685bGPas6",
        "colab_type": "code",
        "outputId": "502b1296-be03-41cf-bcd1-a89ed6c751f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing the Keras wrapper class for the scikit learn function KerasClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'\n",
        "                     ,input_dim=11))\n",
        "    classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'))\n",
        "    classifier.add(Dense(output_dim=1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
        "    \n",
        "    return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn=build_classifier, batch_size=10,epochs=100)\n",
        "accuracies = cross_val_score(classifier,X_train,y=y_train,cv=10,n_jobs=-1,verbose=True)\n",
        "\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  8.7min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiVLUTDCVTYp",
        "colab_type": "code",
        "outputId": "6577b383-c445-4860-dbb4-1953ce193e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Mean of Accuracies',mean)\n",
        "print('Standard Deviation in the accuracies',variance)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of Accuracies 0.8423749952390789\n",
            "Standard Deviation in the accuracies 0.02076242000605802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "urJ3kHA2MIjI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tuning the Parameters\n",
        "\n",
        "We use the GridSearch method from the sklearn.model_selection function for selecting the best hyper parameters for the model."
      ]
    },
    {
      "metadata": {
        "id": "hePlkMf5MHdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'\n",
        "                     ,input_dim=11))\n",
        "    classifier.add(Dense(output_dim=6,kernel_initializer='uniform',activation='relu'))\n",
        "    classifier.add(Dense(output_dim=1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "    classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics =['accuracy'])\n",
        "    \n",
        "    return classifier\n",
        "  \n",
        "classifier = KerasClassifier(build_fn=build_classifier)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3lG2eKORSZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2910
        },
        "outputId": "423c8fde-c4b3-416d-ad92-5a8940398f8e"
      },
      "cell_type": "code",
      "source": [
        "parameters = {'batch_size': [25,32],\n",
        "              'nb_epoch': [50,100],\n",
        "              'optimizer' : ['adam', 'rmsprop']}\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10,verbose=1)\n",
        "grid_search = grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=6)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"sigmoid\", units=1)`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 78us/step - loss: 0.5540 - acc: 0.7971\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 88us/step - loss: 0.5644 - acc: 0.7967\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 96us/step - loss: 0.5724 - acc: 0.7942\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 105us/step - loss: 0.5734 - acc: 0.7964\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 115us/step - loss: 0.5511 - acc: 0.7938\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 124us/step - loss: 0.5636 - acc: 0.7940\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 133us/step - loss: 0.5703 - acc: 0.7965\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 135us/step - loss: 0.5522 - acc: 0.7961\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 146us/step - loss: 0.5627 - acc: 0.7939\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 154us/step - loss: 0.5491 - acc: 0.7961\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 152us/step - loss: 0.5790 - acc: 0.7951\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 161us/step - loss: 0.5834 - acc: 0.7965\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 168us/step - loss: 0.5839 - acc: 0.7940\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 177us/step - loss: 0.5618 - acc: 0.7968\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 182us/step - loss: 0.5689 - acc: 0.7937\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 186us/step - loss: 0.5770 - acc: 0.7917\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 194us/step - loss: 0.5793 - acc: 0.7960\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 197us/step - loss: 0.6160 - acc: 0.7943\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 1s 202us/step - loss: 0.5594 - acc: 0.7953\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 212us/step - loss: 0.5816 - acc: 0.7953\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 224us/step - loss: 0.5603 - acc: 0.7968\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 232us/step - loss: 0.5664 - acc: 0.7960\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 243us/step - loss: 0.5554 - acc: 0.7950\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 247us/step - loss: 0.5777 - acc: 0.7963\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 252us/step - loss: 0.5749 - acc: 0.7917\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 266us/step - loss: 0.5771 - acc: 0.7928\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 276us/step - loss: 0.5706 - acc: 0.7968\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 288us/step - loss: 0.5494 - acc: 0.7953\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 292us/step - loss: 0.5575 - acc: 0.7947\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 295us/step - loss: 0.5685 - acc: 0.7951\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 296us/step - loss: 0.5761 - acc: 0.7972\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 304us/step - loss: 0.5744 - acc: 0.7965\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 308us/step - loss: 0.5759 - acc: 0.7956\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 314us/step - loss: 0.6232 - acc: 0.7957\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 327us/step - loss: 0.6032 - acc: 0.7921\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 335us/step - loss: 0.5676 - acc: 0.7944\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 338us/step - loss: 0.6058 - acc: 0.7944\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 2s 344us/step - loss: 0.5821 - acc: 0.7961\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 349us/step - loss: 0.5657 - acc: 0.7956\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 359us/step - loss: 0.5705 - acc: 0.7957\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 360us/step - loss: 0.5795 - acc: 0.7971\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 367us/step - loss: 0.5926 - acc: 0.7947\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 376us/step - loss: 0.6088 - acc: 0.7919\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 383us/step - loss: 0.5895 - acc: 0.7950\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 397us/step - loss: 0.6266 - acc: 0.7914\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 403us/step - loss: 0.6064 - acc: 0.7911\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 406us/step - loss: 0.6047 - acc: 0.7949\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 423us/step - loss: 0.5838 - acc: 0.7933\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 436us/step - loss: 0.5837 - acc: 0.7949\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 450us/step - loss: 0.5861 - acc: 0.7957\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 438us/step - loss: 0.6074 - acc: 0.7967\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 441us/step - loss: 0.5998 - acc: 0.7958\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 451us/step - loss: 0.6047 - acc: 0.7929\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 452us/step - loss: 0.5918 - acc: 0.7975\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 462us/step - loss: 0.6124 - acc: 0.7929\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 463us/step - loss: 0.6015 - acc: 0.7937\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 474us/step - loss: 0.6240 - acc: 0.7950\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 3s 482us/step - loss: 0.6324 - acc: 0.7944\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 496us/step - loss: 0.6177 - acc: 0.7954\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 506us/step - loss: 0.6279 - acc: 0.7936\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 517us/step - loss: 0.6081 - acc: 0.7947\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 532us/step - loss: 0.5876 - acc: 0.7951\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 537us/step - loss: 0.5865 - acc: 0.7935\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 541us/step - loss: 0.6050 - acc: 0.7956\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 556us/step - loss: 0.6074 - acc: 0.7915\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 564us/step - loss: 0.6030 - acc: 0.7918\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 579us/step - loss: 0.5778 - acc: 0.7967\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 588us/step - loss: 0.5950 - acc: 0.7946\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 591us/step - loss: 0.5894 - acc: 0.7954\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 649us/step - loss: 0.5822 - acc: 0.7954\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 622us/step - loss: 0.5967 - acc: 0.7971\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 4s 621us/step - loss: 0.6406 - acc: 0.7933\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 629us/step - loss: 0.5886 - acc: 0.7951\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 651us/step - loss: 0.6007 - acc: 0.7967\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 660us/step - loss: 0.6238 - acc: 0.7928\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 655us/step - loss: 0.5865 - acc: 0.7944\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 656us/step - loss: 0.5901 - acc: 0.7969\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 665us/step - loss: 0.6612 - acc: 0.7929\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 667us/step - loss: 0.5978 - acc: 0.7947\n",
            "Epoch 1/1\n",
            "7200/7200 [==============================] - 5s 680us/step - loss: 0.5988 - acc: 0.7951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  5.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 5s 671us/step - loss: 0.6531 - acc: 0.7931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lO3du9DYRw3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34ALQs6t5L_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aef70b65-8a78-436a-f465-a55f41418d85"
      },
      "cell_type": "code",
      "source": [
        "print(grid_search.best_params_)\n",
        "print(best_accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 25, 'nb_epoch': 50, 'optimizer': 'adam'}\n",
            "0.796\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}